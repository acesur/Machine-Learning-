{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acesur/Machine-Learning-/blob/main/02_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEXB1ztIhAT",
        "outputId": "8f9edf80-b2f6-4942-b63a-bd898d91a3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/skin-cancer-mnist-ham10000\n",
            "Path 1 exists: True\n",
            "Path 2 exists: True\n",
            "Metadata exists: True\n",
            "Metadata shape: (10015, 7)\n",
            "Metadata columns: ['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization']\n",
            "Sampled metadata distribution:\n",
            "dx\n",
            "akiec    142\n",
            "bcc      142\n",
            "bkl      142\n",
            "mel      142\n",
            "nv       142\n",
            "vasc     142\n",
            "df       115\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1c4b62b0376e>:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  metadata_sampled = metadata.groupby('dx', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size // 7), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 9950/967 images, Success: 136, Failures: 0\n",
            "Processed 10000/967 images, Success: 141, Failures: 0\n",
            "Processed 2750/967 images, Success: 200, Failures: 0\n",
            "Processed 2600/967 images, Success: 202, Failures: 0\n",
            "Processed 2700/967 images, Success: 214, Failures: 0\n",
            "Processed 2900/967 images, Success: 261, Failures: 0\n",
            "Processed 950/967 images, Success: 332, Failures: 0\n",
            "Processed 200/967 images, Success: 363, Failures: 0\n",
            "Processed 400/967 images, Success: 386, Failures: 0\n",
            "Processed 600/967 images, Success: 389, Failures: 0\n",
            "Processed 300/967 images, Success: 405, Failures: 0\n",
            "Processed 50/967 images, Success: 408, Failures: 0\n",
            "Processed 1100/967 images, Success: 428, Failures: 0\n",
            "Processed 1200/967 images, Success: 479, Failures: 0\n",
            "Processed 1150/967 images, Success: 499, Failures: 0\n",
            "Processed 1300/967 images, Success: 553, Failures: 0\n",
            "Processed 1600/967 images, Success: 602, Failures: 0\n",
            "Processed 1350/967 images, Success: 660, Failures: 0\n",
            "Processed 4150/967 images, Success: 724, Failures: 0\n",
            "Processed 5850/967 images, Success: 736, Failures: 0\n",
            "Processed 4350/967 images, Success: 756, Failures: 0\n",
            "Processed 2450/967 images, Success: 927, Failures: 0\n",
            "Processed 2400/967 images, Success: 932, Failures: 0\n",
            "Processed 2350/967 images, Success: 950, Failures: 0\n",
            "Final count - Success: 967, Failures: 0\n",
            "Feature DataFrame shape: (967, 8)\n",
            "Class distribution:\n",
            "dx\n",
            "akiec    142\n",
            "bcc      142\n",
            "bkl      142\n",
            "mel      142\n",
            "nv       142\n",
            "vasc     142\n",
            "df       115\n",
            "Name: count, dtype: int64\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage.measure import regionprops, label\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "IMAGES_PATH_PART1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\"\n",
        "IMAGES_PATH_PART2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
        "METADATA_PATH = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
        "\n",
        "print(f\"Path 1 exists: {os.path.exists(IMAGES_PATH_PART1)}\")\n",
        "print(f\"Path 2 exists: {os.path.exists(IMAGES_PATH_PART2)}\")\n",
        "print(f\"Metadata exists: {os.path.exists(METADATA_PATH)}\")\n",
        "\n",
        "metadata = pd.read_csv(METADATA_PATH)\n",
        "print(f\"Metadata shape: {metadata.shape}\")\n",
        "print(f\"Metadata columns: {metadata.columns.tolist()}\")\n",
        "\n",
        "def find_image_path_improved(image_id, part1_path, part2_path):\n",
        "    if image_id.startswith('ISIC_'):\n",
        "        base_id = image_id\n",
        "        numeric_id = image_id.replace('ISIC_', '')\n",
        "    else:\n",
        "        base_id = f\"ISIC_{image_id}\"\n",
        "        numeric_id = image_id\n",
        "\n",
        "    patterns = [\n",
        "        f\"{base_id}.jpg\",\n",
        "        f\"{numeric_id}.jpg\",\n",
        "        f\"{base_id.lower()}.jpg\",\n",
        "        f\"{numeric_id.lower()}.jpg\"\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        for directory in [part1_path, part2_path]:\n",
        "            path = os.path.join(directory, pattern)\n",
        "            if os.path.exists(path):\n",
        "                return path\n",
        "\n",
        "    for directory in [part1_path, part2_path]:\n",
        "        possible_files = glob(os.path.join(directory, f\"*{base_id}*\"))\n",
        "        if possible_files:\n",
        "            return possible_files[0]\n",
        "        possible_files = glob(os.path.join(directory, f\"*{numeric_id}*\"))\n",
        "        if possible_files:\n",
        "            return possible_files[0]\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_abcd_features_robust(image_path):\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        height, width, _ = img.shape\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        _, thresh_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "        thresh_adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                                cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "        for thresh in [thresh_otsu, thresh_adaptive]:\n",
        "            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            if not contours:\n",
        "                continue\n",
        "\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "            contour_area = cv2.contourArea(largest_contour)\n",
        "            if contour_area < 100:\n",
        "                continue\n",
        "\n",
        "            mask = np.zeros_like(gray)\n",
        "            cv2.drawContours(mask, [largest_contour], 0, 255, -1)\n",
        "\n",
        "            h, w = mask.shape\n",
        "            flipped_mask = cv2.flip(mask, 1)\n",
        "            asymmetry = np.sum(np.abs(mask - flipped_mask)) / (h * w * 255)\n",
        "\n",
        "            perimeter = cv2.arcLength(largest_contour, True)\n",
        "            border_irregularity = perimeter**2 / (4 * np.pi * contour_area) if contour_area > 0 else 0\n",
        "\n",
        "            r, g, b = cv2.split(img)\n",
        "            mask_bool = mask > 0\n",
        "\n",
        "            color_variance_r = np.var(r[mask_bool]) if np.sum(mask_bool) > 0 else 0\n",
        "            color_variance_g = np.var(g[mask_bool]) if np.sum(mask_bool) > 0 else 0\n",
        "            color_variance_b = np.var(b[mask_bool]) if np.sum(mask_bool) > 0 else 0\n",
        "\n",
        "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "            diameter = max(w, h)\n",
        "\n",
        "            return {\n",
        "                'asymmetry': float(asymmetry),\n",
        "                'border_irregularity': float(border_irregularity),\n",
        "                'color_variance_r': float(color_variance_r),\n",
        "                'color_variance_g': float(color_variance_g),\n",
        "                'color_variance_b': float(color_variance_b),\n",
        "                'diameter': float(diameter)\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feature extraction: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Extract features using a diverse stratified sample\n",
        "sample_size = 1000\n",
        "metadata_sampled = metadata.groupby('dx', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size // 7), random_state=42))\n",
        "\n",
        "print(\"Sampled metadata distribution:\")\n",
        "print(metadata_sampled['dx'].value_counts())\n",
        "\n",
        "feature_results = []\n",
        "success_count = 0\n",
        "failure_count = 0\n",
        "\n",
        "for i, row in metadata_sampled.iterrows():\n",
        "    img_id = row['image_id']\n",
        "    img_path = find_image_path_improved(img_id, IMAGES_PATH_PART1, IMAGES_PATH_PART2)\n",
        "    if img_path:\n",
        "        features = extract_abcd_features_robust(img_path)\n",
        "        if features:\n",
        "            features['image_id'] = img_id\n",
        "            features['dx'] = row['dx']\n",
        "            feature_results.append(features)\n",
        "            success_count += 1\n",
        "        else:\n",
        "            failure_count += 1\n",
        "    else:\n",
        "        failure_count += 1\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(metadata_sampled)} images, Success: {success_count}, Failures: {failure_count}\")\n",
        "\n",
        "print(f\"Final count - Success: {success_count}, Failures: {failure_count}\")\n",
        "\n",
        "if feature_results:\n",
        "    feature_df = pd.DataFrame(feature_results)\n",
        "    print(\"Feature DataFrame shape:\", feature_df.shape)\n",
        "    print(\"Class distribution:\")\n",
        "    print(feature_df['dx'].value_counts())\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    feature_df.to_csv('/content/drive/My Drive/abcd_features.csv', index=False)\n",
        "else:\n",
        "    print(\"No features were successfully extracted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5VNa3x7ZKqlC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}